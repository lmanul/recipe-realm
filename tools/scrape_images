#!/usr/bin/python
"""Scrapes images from an existing, JSON list of recipes. This is NOT meant to
be run in production."""

import hashlib
import json
import os
import time

import requests

from bs4 import BeautifulSoup

FETCH_TIMEOUT_SECONDS = 5
COOL_DOWN_TIME_SECONDS = 2
RECIPE_ID_LENGTH = 6
DATA_DIR = "../data"
IMG_DIR = "../img"
TOOLS_DIR = "../tools"

def scrape_img_url_from_markup_bbc(markup):
    "BBC-specific version of a function to get the URL of a recipe's main image"
    soup = BeautifulSoup(markup, "html.parser")
    img_container = soup.find(class_="post-header__image-container")
    img = img_container.find(class_="image__img")
    return img["src"]

def scrape_img_url_from_markup_bbc_middle_east(markup):
    "BBC 'Middle-East' version of a function to get the URL of a recipe's image"
    soup = BeautifulSoup(markup, "html.parser")
    img_container = soup.find(class_="img-holder")
    img = img_container.find(class_="img-responsive")
    return img["src"]

def get_strategy(url):
    "Returns a function suitable for getting the image URL from the given HTML"
    if "www.bbcgoodfood.com" in url:
        return scrape_img_url_from_markup_bbc
    if "www.bbcgoodfoodme.com" in url:
        return scrape_img_url_from_markup_bbc_middle_east
    raise NotImplementedError("I don't support this source yet: " + url)

def scrape_one_recipe_img(url, out_file):
    "Finds the image associated with this recipe URL and saves it to disk"
    response = requests.get(url, timeout=FETCH_TIMEOUT_SECONDS)
    if response.status_code == 200:
        strategy = get_strategy(url)
        img_url = strategy(response.text)
        img_response = requests.get(img_url, timeout=FETCH_TIMEOUT_SECONDS)
        # Show some indication of progress
        print(".", end="", flush=True)
        if img_response.status_code == 200:
            if not os.path.exists(IMG_DIR):
                os.mkdir(IMG_DIR)
            with open(out_file, 'wb') as f:
                f.write(img_response.content)

def get_recipe_id(url):
    "Returns a (reasonably) unique ID from the given recipe URL"
    # A prefix of a SHA-1 is still a reasonable hash.
    return hashlib.sha1(url.encode()).hexdigest()[:RECIPE_ID_LENGTH]

def post_process_img(image_file):
    "Performs any steps required for a fetched image (e.g. filesize reduction)"
    current_dir = os.getcwd()
    os.chdir(TOOLS_DIR)
    os.system("./imgjpg85quality " + image_file)
    os.chdir(current_dir)

def main():
    with open(os.path.join(DATA_DIR, "recipes.json")) as f:
        recipes = json.loads(f.read())
    for recipe in recipes[:1500]:
        if "url" in recipe:
            url = recipe["url"]
            recipe_id = get_recipe_id(url)
            image_file = os.path.join(IMG_DIR, recipe_id + ".jpg")
            if os.path.exists(image_file):
                # TODO: Potentially add a flag to force re-fetch
                print("Already got image, skipping...")
            else:
                scrape_one_recipe_img(url, image_file)
                post_process_img(image_file)
                # Let's not make the scraped website's owner too mad at us
                time.sleep(COOL_DOWN_TIME_SECONDS)

if __name__ == "__main__":
    main()
